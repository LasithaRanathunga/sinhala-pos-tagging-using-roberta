{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167f72dc",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea5b367",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     AutoTokenizer, \n\u001b[32m      6\u001b[39m     AutoModelForTokenClassification,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     DataCollatorForTokenClassification\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Acedemic\\Level 4\\research\\RobeRta-POS\\venv\\Lib\\site-packages\\pandas\\__init__.py:61\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     63\u001b[39m     ArrowDtype,\n\u001b[32m     64\u001b[39m     Int8Dtype,\n\u001b[32m     65\u001b[39m     Int16Dtype,\n\u001b[32m     66\u001b[39m     Int32Dtype,\n\u001b[32m     67\u001b[39m     Int64Dtype,\n\u001b[32m     68\u001b[39m     UInt8Dtype,\n\u001b[32m     69\u001b[39m     UInt16Dtype,\n\u001b[32m     70\u001b[39m     UInt32Dtype,\n\u001b[32m     71\u001b[39m     UInt64Dtype,\n\u001b[32m     72\u001b[39m     Float32Dtype,\n\u001b[32m     73\u001b[39m     Float64Dtype,\n\u001b[32m     74\u001b[39m     CategoricalDtype,\n\u001b[32m     75\u001b[39m     PeriodDtype,\n\u001b[32m     76\u001b[39m     IntervalDtype,\n\u001b[32m     77\u001b[39m     DatetimeTZDtype,\n\u001b[32m     78\u001b[39m     StringDtype,\n\u001b[32m     79\u001b[39m     BooleanDtype,\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     81\u001b[39m     NA,\n\u001b[32m     82\u001b[39m     isna,\n\u001b[32m     83\u001b[39m     isnull,\n\u001b[32m     84\u001b[39m     notna,\n\u001b[32m     85\u001b[39m     notnull,\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     87\u001b[39m     Index,\n\u001b[32m     88\u001b[39m     CategoricalIndex,\n\u001b[32m     89\u001b[39m     RangeIndex,\n\u001b[32m     90\u001b[39m     MultiIndex,\n\u001b[32m     91\u001b[39m     IntervalIndex,\n\u001b[32m     92\u001b[39m     TimedeltaIndex,\n\u001b[32m     93\u001b[39m     DatetimeIndex,\n\u001b[32m     94\u001b[39m     PeriodIndex,\n\u001b[32m     95\u001b[39m     IndexSlice,\n\u001b[32m     96\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     97\u001b[39m     NaT,\n\u001b[32m     98\u001b[39m     Period,\n\u001b[32m     99\u001b[39m     period_range,\n\u001b[32m    100\u001b[39m     Timedelta,\n\u001b[32m    101\u001b[39m     timedelta_range,\n\u001b[32m    102\u001b[39m     Timestamp,\n\u001b[32m    103\u001b[39m     date_range,\n\u001b[32m    104\u001b[39m     bdate_range,\n\u001b[32m    105\u001b[39m     Interval,\n\u001b[32m    106\u001b[39m     interval_range,\n\u001b[32m    107\u001b[39m     DateOffset,\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m    109\u001b[39m     to_numeric,\n\u001b[32m    110\u001b[39m     to_datetime,\n\u001b[32m    111\u001b[39m     to_timedelta,\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    113\u001b[39m     Flags,\n\u001b[32m    114\u001b[39m     Grouper,\n\u001b[32m    115\u001b[39m     factorize,\n\u001b[32m    116\u001b[39m     unique,\n\u001b[32m    117\u001b[39m     value_counts,\n\u001b[32m    118\u001b[39m     NamedAgg,\n\u001b[32m    119\u001b[39m     array,\n\u001b[32m    120\u001b[39m     Categorical,\n\u001b[32m    121\u001b[39m     set_eng_float_format,\n\u001b[32m    122\u001b[39m     Series,\n\u001b[32m    123\u001b[39m     DataFrame,\n\u001b[32m    124\u001b[39m )\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Acedemic\\Level 4\\research\\RobeRta-POS\\venv\\Lib\\site-packages\\pandas\\core\\api.py:28\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     isna,\n\u001b[32m     18\u001b[39m     isnull,\n\u001b[32m     19\u001b[39m     notna,\n\u001b[32m     20\u001b[39m     notnull,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     factorize,\n\u001b[32m     25\u001b[39m     unique,\n\u001b[32m     26\u001b[39m     value_counts,\n\u001b[32m     27\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mboolean\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BooleanDtype\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfloating\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     31\u001b[39m     Float32Dtype,\n\u001b[32m     32\u001b[39m     Float64Dtype,\n\u001b[32m     33\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Acedemic\\Level 4\\research\\RobeRta-POS\\venv\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseArray\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstring_\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringArray\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstring_arrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowStringArray\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtimedeltas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TimedeltaArray\n\u001b[32m     24\u001b[39m __all__ = [\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mArrowExtensionArray\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mExtensionArray\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTimedeltaArray\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1138\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1078\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1504\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1476\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1645\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:161\u001b[39m, in \u001b[36m_path_isfile\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:153\u001b[39m, in \u001b[36m_path_is_mode_type\u001b[39m\u001b[34m(path, mode)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "import wandb  # For experiment tracking\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e357aa3e",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataLoader:\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        \n",
    "    def load_data(self) -> List[Dict]:\n",
    "        dataset = []\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            tokens, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line and tokens:\n",
    "                    dataset.append({\"tokens\": tokens, \"tags\": tags})\n",
    "                    tokens, tags = [], []\n",
    "                elif line:\n",
    "                    try:\n",
    "                        word, tag = line.split()\n",
    "                        tokens.append(word)\n",
    "                        tags.append(tag)\n",
    "                    except ValueError:\n",
    "                        logging.warning(f\"Skipping malformed line: {line}\")\n",
    "            \n",
    "            if tokens:  # Handle last sentence\n",
    "                dataset.append({\"tokens\": tokens, \"tags\": tags})\n",
    "        \n",
    "        logging.info(f\"Loaded {len(dataset)} sentences\")\n",
    "        return dataset\n",
    "\n",
    "    def create_tag_maps(self, data: List[Dict]):\n",
    "        tags = set(tag for example in data for tag in example[\"tags\"])\n",
    "        tag2id = {tag: i for i, tag in enumerate(sorted(tags))}\n",
    "        id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "        return tag2id, id2tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9272fc",
   "metadata": {},
   "source": [
    "### Dataset Creation and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa72f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDatasetProcessor:\n",
    "    def __init__(self, tokenizer_name: str, max_length: int = 128):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def tokenize_and_align_labels(self, examples, tag2id):\n",
    "        tokenized = self.tokenizer(\n",
    "            examples[\"tokens\"],\n",
    "            is_split_into_words=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[\"tags\"]):\n",
    "            word_ids = tokenized.word_ids(i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            \n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(tag2id[label[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "            \n",
    "            labels.append(label_ids)\n",
    "        \n",
    "        tokenized[\"labels\"] = labels\n",
    "        return tokenized\n",
    "\n",
    "    def prepare_dataset(self, data: List[Dict], tag2id: Dict, val_split: float = 0.1, test_split: float = 0.1):\n",
    "        dataset = Dataset.from_dict({\n",
    "            'tokens': [example['tokens'] for example in data],\n",
    "            'tags': [example['tags'] for example in data]\n",
    "        })\n",
    "        \n",
    "        # Create train/val/test splits\n",
    "        splits = dataset.train_test_split(test_size=test_split + val_split)\n",
    "        train_test = splits['train'].train_test_split(test_size=val_split/(1-test_split))\n",
    "        \n",
    "        dataset_dict = DatasetDict({\n",
    "            'train': train_test['train'],\n",
    "            'validation': train_test['test'],\n",
    "            'test': splits['test']\n",
    "        })\n",
    "        \n",
    "        # Tokenize datasets\n",
    "        tokenized_datasets = dataset_dict.map(\n",
    "            lambda x: self.tokenize_and_align_labels(x, tag2id),\n",
    "            batched=True,\n",
    "            remove_columns=dataset_dict['train'].column_names\n",
    "        )\n",
    "        \n",
    "        return tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb3bc70",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTrainer:\n",
    "    def __init__(self, model_name: str, num_labels: int, id2tag: Dict, tag2id: Dict):\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            id2label=id2tag,\n",
    "            label2id=tag2id\n",
    "        )\n",
    "        \n",
    "        self.training_args = TrainingArguments(\n",
    "            output_dir=\"./sinhala-pos-model\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=100,\n",
    "            logging_steps=100,\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=5,\n",
    "            weight_decay=0.01,\n",
    "            warmup_ratio=0.1,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            save_total_limit=2,\n",
    "            report_to=\"wandb\"\n",
    "        )\n",
    "        \n",
    "    def compute_metrics(self, p):\n",
    "        predictions = np.argmax(p.predictions, axis=2)\n",
    "        labels = p.label_ids\n",
    "        \n",
    "        true_predictions = [\n",
    "            [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        \n",
    "        results = {\n",
    "            \"accuracy\": accuracy_score(sum(true_labels, []), sum(true_predictions, [])),\n",
    "        }\n",
    "        \n",
    "        # Add detailed classification metrics\n",
    "        report = classification_report(\n",
    "            sum(true_labels, []), \n",
    "            sum(true_predictions, []), \n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        results.update({\n",
    "            f\"f1_{k}\": v['f1-score']\n",
    "            for k, v in report.items()\n",
    "            if k not in ['accuracy', 'macro avg', 'weighted avg']\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def train(self, tokenized_datasets, tokenizer):\n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"],\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e510a89",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d0f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "API key must be 40 characters long, yours was 12",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Initialize wandb\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msinhala-pos-tagger\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m      6\u001b[39m     loader = POSDataLoader(\u001b[33m\"\u001b[39m\u001b[33msinhala_pos.txt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Acedemic\\Level 4\\research\\RobeRta-POS\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1623\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[39m\n\u001b[32m   1619\u001b[39m     wl._get_logger().exception(\u001b[33m\"\u001b[39m\u001b[33merror in wandb.init()\u001b[39m\u001b[33m\"\u001b[39m, exc_info=e)\n\u001b[32m   1621\u001b[39m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[32m   1622\u001b[39m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1623\u001b[39m \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Acedemic\\Level 4\\research\\RobeRta-POS\\venv\\Lib\\site-packages\\wandb\\analytics\\sentry.py:156\u001b[39m, in \u001b[36mSentry.reraise\u001b[39m\u001b[34m(self, exc)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28mself\u001b[39m.exception(exc)\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc.with_traceback(sys.exc_info()[\u001b[32m2\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Acedemic\\Level 4\\research\\RobeRta-POS\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1551\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[39m\n\u001b[32m   1547\u001b[39m wl = wandb_setup.singleton()\n\u001b[32m   1549\u001b[39m wi = _WandbInit(wl, init_telemetry)\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m \u001b[43mwi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaybe_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m run_settings, show_warnings = wi.make_run_settings(init_settings)\n\u001b[32m   1554\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(run_settings.reinit, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Acedemic\\Level 4\\research\\RobeRta-POS\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:191\u001b[39m, in \u001b[36m_WandbInit.maybe_login\u001b[39m\u001b[34m(self, init_settings)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_settings._noop \u001b[38;5;129;01mor\u001b[39;00m run_settings._offline:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[43mwandb_login\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43manonymous\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[43m.\u001b[49m\u001b[43manonymous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_disable_warning\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_silent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Acedemic\\Level 4\\research\\RobeRta-POS\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_login.py:321\u001b[39m, in \u001b[36m_login\u001b[39m\u001b[34m(anonymous, key, relogin, host, force, timeout, verify, referrer, _silent, _disable_warning)\u001b[39m\n\u001b[32m    318\u001b[39m     wlogin._verify_login(key)\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key_is_pre_configured:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     \u001b[43mwlogin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtry_save_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     wlogin.update_session(key, status=key_status)\n\u001b[32m    323\u001b[39m     wlogin._update_global_anonymous_setting()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Acedemic\\Level 4\\research\\RobeRta-POS\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_login.py:180\u001b[39m, in \u001b[36m_WandbLogin.try_save_api_key\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[43mapikey\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m apikey.WriteNetrcError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    182\u001b[39m         wandb.termwarn(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Acedemic\\Level 4\\research\\RobeRta-POS\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\apikey.py:313\u001b[39m, in \u001b[36mwrite_key\u001b[39m\u001b[34m(settings, key, api)\u001b[39m\n\u001b[32m    310\u001b[39m _, suffix = key.split(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;28;01melse\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, key)\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(suffix) != \u001b[32m40\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI key must be 40 characters long, yours was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    315\u001b[39m write_netrc(settings.base_url, \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, key)\n",
      "\u001b[31mValueError\u001b[39m: API key must be 40 characters long, yours was 12"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"sinhala-pos-tagger\")\n",
    "    \n",
    "    # Load data\n",
    "    loader = POSDataLoader(\"sinhala_pos.txt\")\n",
    "    data = loader.load_data()\n",
    "    tag2id, id2tag = loader.create_tag_maps(data)\n",
    "    \n",
    "    # Process dataset\n",
    "    processor = POSDatasetProcessor(\"xlm-roberta-base\")\n",
    "    tokenized_datasets = processor.prepare_dataset(data, tag2id)\n",
    "    \n",
    "    # Train model\n",
    "    trainer = POSTrainer(\n",
    "        model_name=\"xlm-roberta-base\",\n",
    "        num_labels=len(tag2id),\n",
    "        id2tag=id2tag,\n",
    "        tag2id=tag2id\n",
    "    )\n",
    "    \n",
    "    trained_trainer = trainer.train(tokenized_datasets, processor.tokenizer)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = trained_trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "    print(\"Final Evaluation Results:\", results)\n",
    "    \n",
    "    # Save model\n",
    "    trained_trainer.save_model(\"final-sinhala-pos-model\")\n",
    "    processor.tokenizer.save_pretrained(\"final-sinhala-pos-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d48ae",
   "metadata": {},
   "source": [
    "### Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos(text: str, model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    \n",
    "    results = []\n",
    "    word_ids = inputs.word_ids()\n",
    "    for token, prediction in zip(tokens, predictions[0][1:-1]):  # Skip special tokens\n",
    "        tag = model.config.id2label[prediction.item()]\n",
    "        results.append((token, tag))\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
